{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Report - Wrangle tweet archive of Twitter user @dog_rates, also known as WeRateDogs.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Gather](#gather)\n",
    "- [Assess](#assess)\n",
    "- [Clean](#clean)\n",
    "- [Store](#store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report describes the steps involved in data wrangling for the tweet archive of Twitter user,WeRateDogs.\n",
    "Data wrangling constitutes following steps:\n",
    "\n",
    "1. Gather\n",
    "\n",
    "2. Assess\n",
    "\n",
    "3. Clean\n",
    "\n",
    "4. Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following data needs to be gathered  -\n",
    "\n",
    "a. Twitter archive data from WeRateDogs consisting of basic data has been used to extract fields and compiled in to 'twitter_archive_enhanced.csv'.This file is to be downloaded.This being a ready file has data extracted for rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo).The data extracted is based on ratings.\n",
    "\n",
    "b. Image Predictions File consists of image prediction for each tweet ID, image URL, and the image number that corresponded to the most confident prediction. The has been obtained by running the archive data through the neural network.\n",
    "\n",
    "c. Downloading addition data from twitter archive for retweet count and favorite count using twitter id from the file provided in point 1.This is done by web scraping using Twitter's API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data mentioned aboved needs to be loaded in to jupyter notebook and this is achieved by importing the required libraries to read in the data.Post importing the libraries, follow the below steps :\n",
    "\n",
    "1. Load the dataset provided in the file - *twitter_archive_enhanced.csv* to dataframe : *dog_ratings*\n",
    "2. Download the image predictions  file programmitically from the url provided -URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv.\n",
    "3. This file needs to be read into a datframe - img_predictions\n",
    "4. Inorder to download additional data, we need to query Twitter's API.\n",
    "5. Pre-requisites to query data from Twitter API  include -Twitter account and Twitter application.Hence setup a Twitter application and create an API object that will be used to gather Twitter data.\n",
    "6. Download additional data retweet count and favourite count from Twitter Archive. The data downloaded is against the twitter id from the file provided(point 1).The data is extracted in json format and stored in a file.The file that stores the json object in each line is called  tweet_json.txt.\n",
    "8. Read in the  tweet_json.txt into a dataframe - tweet_data_info. Start by creating an empty dataframe and add columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "At the end of gather stage, we have three dataframes\n",
    "1. Enhanced twitter data\n",
    "       dog_ratings\n",
    "2. Image predictions data\n",
    "       img_predictions\n",
    "3. Additional twitter data\n",
    "       tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess step involves inspecting data visually and programmitically to identify quality and tideness issues.The project requirement stipulates that atleast 8 Quality issues and 2 Tidiness issues are identified and cleaned.Assessment of the data with respect to quality and tidiness are listed below-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Quality Issues**\n",
    "\n",
    "1. NaN values and non null values in the columns - 'retweeted_status_id', 'retweeted_status_user_id', and 'retweeted_status_timestamp'\n",
    "\n",
    "2. Incorrect datatype for columns :in_reply_to_status_id, in_reply_to_user_id and timestamp\n",
    "\n",
    "3. Non descriptive column headers in dog_ratings dataframe:  doggo,floofer,pupper,puppo and names are not intuitive.\n",
    "\n",
    "4. Not all tweets have dog name(s) captured.Many non-name entries like a, None,officially,quite,etc are included.\n",
    "\n",
    "5. The columns :doggo,floofer,pupper,puppo indicates dog stage for the supposed dog mentioned in the tweet but there are tweet_ids where no dog stage has been captured and some of the tweets have more than one dog stage captured for the same mention.Also the column header names are non descriptive.\n",
    "\n",
    "6. Source can be made explicit instead of the html string, from a analysis and visualization perspective.\n",
    "\n",
    "7. Tweet id count in the img_predictions does not tally with that of the enhanced twitter data-Original tweets from enhanced twitter data to be matched against img_predictions indicating we need original tweets with images only.\n",
    "\n",
    "8. Non descriptive column header -'jpg_urls' in image predictions data.\n",
    "\n",
    "9. Inconsistent case -the dog breeds under p1,p2and p3 prediction columns have dog breeds in title case and also in lower case.\n",
    "\n",
    "10. Mismatch in the tweet id count between enhanced twitter data and additional twitter data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Tideness issues**\n",
    "\n",
    "1. Variables :doggo,floofer,pupper and puppo  can be categorized under 'dog stages'.\n",
    "\n",
    "2. enhanced twitter data and additional twitter data captured under dog_ratings_clean and tweet_data_df respectively can be merged.\n",
    "\n",
    "3. Timestamp column can be split into date and time columns.\n",
    "\n",
    "4. Dog breeds captured in img_predictions dataframe under p1,p2,p3 can be categorised under a  single column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='clean'></a>\n",
    "## Clean\n",
    "\n",
    "Quality and Tideness issues identified under Assess section needs to be actioned by 'Defining' what and how the cleaning has to be done.This is translated in to 'Code' and this follows 'Testing' the 'Code' changes to clean the identified issues. Before begining the cleaning action, copies of the datadrames from Assess section needs to be made which will then  be worked on.\n",
    "Therefore for every issues identified will be - \n",
    "1. Elaborate the fix under **Define**\n",
    "2. Translate the fix into **Code**\n",
    "3. Test the code changes under **Test**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='store'></a>\n",
    "## Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the final dataframe to a csv file  *twitter_archive_master.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of steps detailed above needs to be read in conjunction with **wrangle_act.ipynb**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:udacity_ipnd]",
   "language": "python",
   "name": "conda-env-udacity_ipnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
